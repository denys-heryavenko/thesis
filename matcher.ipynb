{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ccfa1a0-85f6-45da-a3c7-e1855444edb0",
   "metadata": {},
   "source": [
    "# Entity Linking\n",
    "This EL script works on two levels: \n",
    "1) Extracted phrases matched to ESCO skill labels.\n",
    "2) Extracted units matched to ESCO skill descriptions.\n",
    "\n",
    "The final output is a table that stores all the matches for each level, as well as the aggregated *final_meta* column, which contains the top matches from both levels and additional information about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "861247c4-c5b3-4c71-8459-dfc81122393d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs that are going to be matched: All Jobs\n",
      "Random number of jobs to be matched (if present): None\n",
      "Top-k matches per skillType on phrase level: 20\n",
      "Top-k matches per skillType on unit level: 20\n",
      "Final number of ESCO matches in the aggregated list: 40\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "\n",
    "import config as C\n",
    "from utils import get_stopwords_de, tokenize\n",
    "\n",
    "\n",
    "SQLITE_PATH = Path(C.SQLITE_PATH)\n",
    "\n",
    "JOB_IDS: List[int] | None = C.JOB_IDS\n",
    "SAMPLE_JOBS = C.SAMPLE_JOBS\n",
    "TOP_K_PHRASE = C.TOP_K_PHRASE\n",
    "MAX_UNIT_MATCHES = C.MAX_UNIT_MATCHES\n",
    "FINAL_TOP = C.FINAL_TOP\n",
    "\n",
    "W_SEMANTIC = C.W_SEMANTIC\n",
    "W_LEXICAL = C.W_LEXICAL\n",
    "W_CONTEXT = C.W_CONTEXT\n",
    "\n",
    "l = None\n",
    "if not JOB_IDS:\n",
    "    l = \"All Jobs\"\n",
    "else: \n",
    "    l = JOB_IDS\n",
    "\n",
    "print(\"Jobs that are going to be matched:\", l)\n",
    "print(\"Random number of jobs to be matched (if present):\", SAMPLE_JOBS)\n",
    "print(\"Top-k matches per skillType on phrase level:\", TOP_K_PHRASE)\n",
    "print(\"Top-k matches per skillType on unit level:\", MAX_UNIT_MATCHES)\n",
    "print(\"Final number of ESCO matches in the aggregated list:\", FINAL_TOP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35779413-ea7b-4ca3-a3ef-3c17e102fc59",
   "metadata": {},
   "source": [
    "### Before we procees, additional helper functions need to be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1e2d363-d675-45c4-9b1a-698b23582a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def add_percentile_norm(items, score_key=\"score\", out_key=\"norm\"): #we need to normalize the similarity scores for each level, to then rank them together and form the final_meta column\n",
    "    \n",
    "    #lowest score - small percentile\n",
    "    #highest score - percentile close to 1\n",
    "\n",
    "    if not items:\n",
    "        return items\n",
    "\n",
    "    scores = []\n",
    "    for x in items:\n",
    "        value = x.get(score_key, 0.0)\n",
    "        scores.append(float(value))\n",
    "\n",
    "    values = np.array(scores, dtype = float) #convert to np array\n",
    "    order = np.argsort(values)\n",
    "    norm = np.empty_like(values, dtype = float)\n",
    "\n",
    "    #assign the percentile\n",
    "    length = float(len(values))\n",
    "    for rank, idx in enumerate(order):\n",
    "        norm[idx] = (rank + 1) / length\n",
    "\n",
    "    for i in range(len(items)):\n",
    "        items[i][out_key] = float(norm[i])\n",
    "\n",
    "    return items\n",
    "\n",
    "\n",
    "def jaccard(a, b): #jaccard similarity between two sets\n",
    "    \n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "\n",
    "    inter = len(a&b)\n",
    "    union = len(a|b)\n",
    "\n",
    "    if inter == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return inter / float(union)\n",
    "\n",
    "\n",
    "def parse_vec(json_str): #from json to np array\n",
    "    \n",
    "    data = json.loads(json_str)\n",
    "    return np.asarray(data, dtype=np.float32)\n",
    "\n",
    "\n",
    "def normalize_rows(mat): #I normalize to keep the vectors consistent after conversions and storage\n",
    "    \n",
    "    if mat.size == 0:\n",
    "        return mat\n",
    "\n",
    "    norms = np.linalg.norm(mat, axis=1, keepdims=True) #compute the L2 norm of each row vector\n",
    "    norms = np.maximum(norms, 1e-12) #to prevent division by 0, we replace zero length with a small number\n",
    "\n",
    "    return mat/norms\n",
    "\n",
    "def to_py_int_or_none(x): #helper function that converts numpy/pandas int-like values to plain python int (used for char offset numbers)\n",
    "    \n",
    "    if x is None:\n",
    "        return None\n",
    "    try:\n",
    "        import math\n",
    "        if isinstance(x, float) and math.isnan(x):\n",
    "            return None\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def load_table(conn, name):\n",
    "    \n",
    "    query = \"SELECT * FROM \" + name\n",
    "    return pd.read_sql_query(query, conn)\n",
    "\n",
    "\n",
    "def greedy_funk(items, key=\"label\"): #remove duplicates based on string field\n",
    "   \n",
    "    seen = set()\n",
    "    result = []\n",
    "\n",
    "    for t in items:\n",
    "        value = t.get(key)\n",
    "        if value is None:\n",
    "            continue\n",
    "\n",
    "        value_str = str(value).strip().lower()\n",
    "        if value_str == \"\":\n",
    "            continue\n",
    "\n",
    "        if value_str not in seen:\n",
    "            seen.add(value_str)\n",
    "            result.append(t)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def find_char_span(text, fragment): #start and end index of a matched phrase/unit/word inside the description. \n",
    "    \n",
    "    if not text or not fragment:\n",
    "        return None\n",
    "\n",
    "    text_low = text.lower()\n",
    "    frag_low = fragment.lower()\n",
    "\n",
    "    idx = text_low.find(frag_low)\n",
    "    if idx == -1:\n",
    "        return None\n",
    "\n",
    "    start = int(idx)\n",
    "    end = int(idx + len(fragment))\n",
    "\n",
    "    return {\"start\": start, \"end\": end}\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b48c1b1-577f-4c5a-ab50-984d2751983e",
   "metadata": {},
   "source": [
    "### Load all needed tables from SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f7ec43e-ebeb-47dc-888c-c94aa690779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(str(SQLITE_PATH))\n",
    "\n",
    "jobs_clean = load_table(conn, \"jobs_clean\")\n",
    "job_phr = load_table(conn, \"job_phrases\")\n",
    "job_units = load_table(conn, \"job_units\")\n",
    "esco_labels = load_table(conn, \"esco_labels\")\n",
    "esco_desc = load_table(conn, \"esco_desc\")\n",
    "esco_skills = load_table(conn, \"esco_skills\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "jobs = jobs_clean[[\"job_id\", \"title\", \"text_deduped\"]].copy()\n",
    "job_phr = job_phr[[\"job_id\", \"phrase_surface\", \"phrase_start\", \"phrase_end\", \"embedding_json\"]].copy()\n",
    "job_units = job_units[[\"job_id\", \"unit_text\", \"unit_start\", \"unit_end\", \"embedding_json\"]].copy()\n",
    "esco_labels = esco_labels[[\"esco_id\", \"label\", \"skillType\", \"embedding_json\"]].copy()\n",
    "esco_desc = esco_desc[[\"esco_id\", \"description\", \"skillType\", \"embedding_json\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99e6304-402f-43a5-98e6-4512ee7134e0",
   "metadata": {},
   "source": [
    "### Filter by Job ID or Sample\n",
    "(If specific/random jobs are to be put through the pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48a3ce7c-34be-459b-8f5f-4df8c676b7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected job_ids: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400]\n",
      "Number of selected jobs: 400\n"
     ]
    }
   ],
   "source": [
    "selected_jobs = []\n",
    "\n",
    "if JOB_IDS is not None and len(JOB_IDS) > 0:\n",
    "    for i in range(len(jobs)):\n",
    "        jid = jobs.loc[i, \"job_id\"]\n",
    "        if jid in JOB_IDS:\n",
    "            selected_jobs.append(i)\n",
    "\n",
    "    #keep only the selected rows\n",
    "    jobs = jobs.iloc[selected_jobs].copy()\n",
    "    jobs = jobs.reset_index(drop=True)\n",
    "\n",
    "else:\n",
    "    #if random job selection\n",
    "    if SAMPLE_JOBS is not None and SAMPLE_JOBS > 0:\n",
    "        \n",
    "        max_jobs = len(jobs)\n",
    "        how_many = min(SAMPLE_JOBS, max_jobs)\n",
    "\n",
    "        random_indices = random.sample(range(max_jobs), how_many)\n",
    "\n",
    "        jobs = jobs.iloc[random_indices].copy()\n",
    "        jobs = jobs.reset_index(drop=True)\n",
    "\n",
    "#otherwise we keep all jobs\n",
    "\n",
    "job_id_list = []\n",
    "for i in range(len(jobs)):\n",
    "    job_id_list.append(jobs.loc[i, \"job_id\"])\n",
    "\n",
    "print(\"Selected job_ids:\", job_id_list)\n",
    "print(\"Number of selected jobs:\", len(jobs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f551d75-1489-4dad-b841-813f264e78c7",
   "metadata": {},
   "source": [
    "### Prepare stopwords and ESCO tokens\n",
    "This step is done to support the hybrid matching model that uses lexical overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92a13003-de52-487e-9d05-cabd31054fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'de_core_news_sm' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>label_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Musikpersonal verwalten</td>\n",
       "      <td>{verwalten, musikpersonal}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Strafvollzugsverfahren beaufsichtigen</td>\n",
       "      <td>{beaufsichtigen, strafvollzugsverfahren}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nicht unterdrückende Praktiken anwenden</td>\n",
       "      <td>{anwenden, unterdrückende, praktiken}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Einhaltung von Vorschriften von Eisenbahnfahrz...</td>\n",
       "      <td>{überprüfen, einhaltung, vorschriften, eisenba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>verfügbare Dienste ermitteln</td>\n",
       "      <td>{ermitteln, verfügbare, dienste}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               label  \\\n",
       "0                            Musikpersonal verwalten   \n",
       "1              Strafvollzugsverfahren beaufsichtigen   \n",
       "2            nicht unterdrückende Praktiken anwenden   \n",
       "3  Einhaltung von Vorschriften von Eisenbahnfahrz...   \n",
       "4                       verfügbare Dienste ermitteln   \n",
       "\n",
       "                                        label_tokens  \n",
       "0                         {verwalten, musikpersonal}  \n",
       "1           {beaufsichtigen, strafvollzugsverfahren}  \n",
       "2              {anwenden, unterdrückende, praktiken}  \n",
       "3  {überprüfen, einhaltung, vorschriften, eisenba...  \n",
       "4                   {ermitteln, verfügbare, dienste}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "STOP_DE = set(get_stopwords_de())\n",
    "label_tokens_list = []\n",
    "\n",
    "#one by one through the esco labels list\n",
    "for i in range(len(esco_labels)):\n",
    "    text = esco_labels.loc[i, \"label\"]\n",
    "    \n",
    "    if not isinstance(text, str): #double check:)\n",
    "        text = \"\"\n",
    "\n",
    "    tokens = tokenize(text, stopwords=STOP_DE)\n",
    "    label_tokens_list.append(set(tokens))\n",
    "\n",
    "\n",
    "esco_labels[\"label_tokens\"] = label_tokens_list\n",
    "display(esco_labels[[\"label\", \"label_tokens\"]].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b923cf-6237-4476-b873-faeb1e2c02fe",
   "metadata": {},
   "source": [
    "### Split ESCO into knowledge vs skill, build embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d1f54f3-d10a-44bc-b4f9-1b34027b9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#each list will store either knowledge or skill concepts\n",
    "#we start with the labels\n",
    "TF_kn_lbl = []\n",
    "\n",
    "for i in range(len(esco_labels)):\n",
    "    st = esco_labels.loc[i, \"skillType\"]\n",
    "    st_str = str(st).lower()\n",
    "    if \"knowledge\" in st_str:\n",
    "        TF_kn_lbl.append(True)\n",
    "    else:\n",
    "        TF_kn_lbl.append(False)\n",
    "\n",
    "lbl_kn = esco_labels[TF_kn_lbl].copy() #store knowledge labels\n",
    "lbl_kn = lbl_kn.reset_index(drop=True)\n",
    "\n",
    "lbl_sk = esco_labels[[not t for t in TF_kn_lbl]].copy() #store skill labels\n",
    "lbl_sk = lbl_sk.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fe359f0-4f14-4baf-923b-02107cee7064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: knowledge = 3219 skill = 10720\n",
      "Desc: knowledge = 3219 skill = 10720\n"
     ]
    }
   ],
   "source": [
    "#same thing with descriptions\n",
    "#however, although we match descriptions with units, we output the corresponding label, as it will be easier to interpret and compare with phrase-level results\n",
    "\n",
    "#for this, there is a small lookup table\n",
    "id_to_label = {}\n",
    "for i in range(len(esco_skills)):\n",
    "    esco_id_value = esco_skills.loc[i, \"esco_id\"]\n",
    "    label_value = esco_skills.loc[i, \"label\"]\n",
    "    id_to_label[esco_id_value] = label_value\n",
    "\n",
    "\n",
    "\n",
    "TF_kn_desc = []\n",
    "for i in range(len(esco_desc)):\n",
    "    st = esco_desc.loc[i, \"skillType\"]\n",
    "    st_str = str(st).lower()\n",
    "    if \"knowledge\" in st_str:\n",
    "        TF_kn_desc.append(True)\n",
    "    else:\n",
    "        TF_kn_desc.append(False)\n",
    "\n",
    "desc_kn = esco_desc[TF_kn_desc].copy()\n",
    "desc_kn = desc_kn.reset_index(drop=True)\n",
    "\n",
    "desc_sk = esco_desc[[not x for x in TF_kn_desc]].copy()\n",
    "desc_sk = desc_sk.reset_index(drop=True)\n",
    "\n",
    "#we add the label column to desc_kn and desc_sk\n",
    "label_list_kn = []\n",
    "for i in range(len(desc_kn)):\n",
    "    esco_id_value = desc_kn.loc[i, \"esco_id\"]\n",
    "    label_value = id_to_label.get(esco_id_value, \"\")\n",
    "    label_list_kn.append(label_value)\n",
    "desc_kn[\"label\"] = label_list_kn\n",
    "\n",
    "label_list_sk = []\n",
    "for i in range(len(desc_sk)):\n",
    "    esco_id_value = desc_sk.loc[i, \"esco_id\"]\n",
    "    label_value = id_to_label.get(esco_id_value, \"\")\n",
    "    label_list_sk.append(label_value)\n",
    "desc_sk[\"label\"] = label_list_sk\n",
    "\n",
    "print(\"Label: knowledge =\", len(lbl_kn), \"skill =\", len(lbl_sk))\n",
    "print(\"Desc: knowledge =\", len(desc_kn), \"skill =\", len(desc_sk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6ff9699-6c88-462e-9f8d-a8a4597eeb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out the embedding dimensions from row 1\n",
    "if len(esco_labels) > 0:\n",
    "    first_emb_str = esco_labels.loc[0, \"embedding_json\"]\n",
    "    example_vec = parse_vec(first_emb_str)\n",
    "    EMB_DIM = example_vec.shape[0]\n",
    "else:\n",
    "    EMB_DIM = 384  #fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6682b121-6f46-4255-880f-64f22955c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper matrix function\n",
    "def build_embedding_matrix(df, col_name=\"embedding_json\"):\n",
    "    \n",
    "    vec_list = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        emb_str = df.loc[i, col_name]\n",
    "        vec = parse_vec(emb_str)\n",
    "        vec_list.append(vec)\n",
    "\n",
    "    #build the matrix and normalize\n",
    "    mat = np.vstack(vec_list)\n",
    "    mat_norm = normalize_rows(mat)\n",
    "    return mat_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f34d139-498b-4475-a426-d5fd82248578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mx_lbl_kn = (3219, 384)\n",
      "mx_lbl_sk = (10720, 384)\n",
      "mx_desc_kn = (3219, 384)\n",
      "mx_desc_sk = (10720, 384)\n"
     ]
    }
   ],
   "source": [
    "#build the matrix\n",
    "\n",
    "mx_lbl_kn = build_embedding_matrix(lbl_kn, \"embedding_json\")\n",
    "mx_lbl_sk = build_embedding_matrix(lbl_sk, \"embedding_json\")\n",
    "mx_desc_kn = build_embedding_matrix(desc_kn, \"embedding_json\")\n",
    "mx_desc_sk = build_embedding_matrix(desc_sk, \"embedding_json\")\n",
    "\n",
    "\n",
    "print(\"mx_lbl_kn =\", mx_lbl_kn.shape)\n",
    "print(\"mx_lbl_sk =\", mx_lbl_sk.shape)\n",
    "print(\"mx_desc_kn =\", mx_desc_kn.shape)\n",
    "print(\"mx_desc_sk =\", mx_desc_sk.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5320b331-fc64-41d2-8cf6-255fc9b96408",
   "metadata": {},
   "source": [
    "### Build phrase and units lists per job\n",
    "- On Phrase level: job_id -> list of (text, vector, token_set)\n",
    "- On Bullet level: job_id -> list of (text, vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fe7d9e6-fdbf-455c-9737-cc32b51c30ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobid_to_phr = {}\n",
    "jobid_to_units = {}\n",
    "\n",
    "#phrase level\n",
    "for i in range(len(job_phr)):\n",
    "    row = job_phr.loc[i]\n",
    "    jid = row[\"job_id\"]\n",
    "    txt = row[\"phrase_surface\"]\n",
    "    txt = txt.strip()\n",
    "    \n",
    "    emb_str = row[\"embedding_json\"]\n",
    "    vec = parse_vec(emb_str) #convert json embedding to a vector\n",
    "\n",
    "    tok_list = tokenize(txt, stopwords=STOP_DE) #tokenize phrase text\n",
    "    tok_set = set(tok_list)\n",
    "\n",
    "    #get the previously computed span from preprocessing\n",
    "    p_start = row.get(\"phrase_start\", None)\n",
    "    p_end = row.get(\"phrase_end\", None)\n",
    "\n",
    "    if jid not in jobid_to_phr:\n",
    "        jobid_to_phr[jid] = []\n",
    "\n",
    "    jobid_to_phr[jid].append((txt, vec, tok_set, p_start, p_end))\n",
    "\n",
    "\n",
    "#unit level\n",
    "\n",
    "for i in range(len(job_units)):\n",
    "    row = job_units.loc[i]\n",
    "    jid = row[\"job_id\"]\n",
    "    txt = row[\"unit_text\"]\n",
    "    txt = txt.strip()\n",
    "\n",
    "    emb_str = row[\"embedding_json\"]\n",
    "    vec = parse_vec(emb_str)\n",
    "\n",
    "    #read the span from preprocessing\n",
    "    u_start = row.get(\"unit_start\", None)\n",
    "    u_end = row.get(\"unit_end\", None)\n",
    "\n",
    "    if jid not in jobid_to_units:\n",
    "        jobid_to_units[jid] = []\n",
    "\n",
    "    jobid_to_units[jid].append((txt, vec, u_start, u_end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2602b103-a5aa-48cb-8848-efbda57c3527",
   "metadata": {},
   "source": [
    "### Hybrid Scoring function (phrase level)\n",
    "Score ESCO labels for one job using:\n",
    "- semantic similarity (cosine sim via dot product)\n",
    "- lexical Jaccard (phrase tokens vs label tokens)\n",
    "- context Jaccard (title/phrase-union vs label tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b392c55-fd68-4770-bc8d-ff670422e459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_bucket_labels(\n",
    "    Phr_emb, #phrase embeddings (matrix)\n",
    "    phr_txt, #list of phrase strings\n",
    "    phr_tok_list, #list of sets of tokens for each phrase\n",
    "    phr_tok_union,\n",
    "    title_tok,\n",
    "    Esco_lbl_emb, #ESCO label embeddings (matrix)\n",
    "    esco_lbl_df, #ESCO labels dataframe\n",
    "    phr_spans  #list of (start, end) per phrase\n",
    "):\n",
    "\n",
    "    #semantic similarity\n",
    "\n",
    "    # S is a matrix: rows = phrases, columns = ESCO labels\n",
    "    #each value is the dot product between phrase embedding and label embedding\n",
    "    S = np.dot(Phr_emb, Esco_lbl_emb.T)\n",
    "\n",
    "    #for each ESCO label we find the best semantic score and the index of the phrase that gives that score\n",
    "    sem_scores = S.max(axis=0)\n",
    "    best_phrase_idx = S.argmax(axis=0)\n",
    "\n",
    "    #lexical Jaccard\n",
    "    n_labels = Esco_lbl_emb.shape[0]\n",
    "    lex_scores = np.zeros(n_labels, dtype=np.float32)\n",
    "\n",
    "    for j in range(n_labels):\n",
    "        label_tokens = esco_lbl_df.iloc[j][\"label_tokens\"] #label_tokens is a set of tokens for ESCO label j\n",
    "\n",
    "        best_lex = 0.0\n",
    "\n",
    "        #then compare this label to every phrase token set\n",
    "        for phrase_tokens in phr_tok_list:\n",
    "            v = jaccard(label_tokens, phrase_tokens)\n",
    "            if v > best_lex:\n",
    "                best_lex = v\n",
    "\n",
    "        lex_scores[j] = best_lex\n",
    "\n",
    "    #context Jaccard\n",
    "    ctx_scores = np.zeros(n_labels, dtype=np.float32)\n",
    "\n",
    "    for j in range(n_labels):\n",
    "        label_tokens = esco_lbl_df.iloc[j][\"label_tokens\"]\n",
    "        \n",
    "        a = jaccard(label_tokens, title_tok)  #compare label tokens with job title tokens\n",
    "        b = jaccard(label_tokens, phr_tok_union) #compare label tokens with union of all phrase tokens\n",
    "\n",
    "        #take the better of the two\n",
    "        ctx_scores[j] = max(a, b)\n",
    "\n",
    "    #and now we combine scores using the predefined weights\n",
    "    combined_score = (W_SEMANTIC * sem_scores + W_LEXICAL  * lex_scores + W_CONTEXT  * ctx_scores)\n",
    "\n",
    "    #pick the top k labels\n",
    "    k = min(TOP_K_PHRASE, len(combined_score))\n",
    "\n",
    "    #get the indices of labels sorted by score\n",
    "    sorted_idx = np.argsort(-combined_score)  #descending\n",
    "    top_idx = sorted_idx[:k]\n",
    "\n",
    "    #output list\n",
    "    results = []\n",
    "\n",
    "    for j in top_idx:\n",
    "        #j is the label index in esco_df\n",
    "        label_text = esco_lbl_df.iloc[j][\"label\"]\n",
    "        skill_type = str(esco_lbl_df.iloc[j][\"skillType\"])\n",
    "        score_value = float(combined_score[j])\n",
    "\n",
    "        #best phrase index for this label\n",
    "        if len(phr_txt) > 0:\n",
    "            p_idx = int(best_phrase_idx[j])\n",
    "            matched_phrase = phr_txt[p_idx]\n",
    "\n",
    "            #get the stored span for that phrase (may be None,None)\n",
    "            if p_idx < len(phr_spans):\n",
    "                raw_start, raw_end = phr_spans[p_idx]\n",
    "            else:\n",
    "                raw_start, raw_end = (None, None)\n",
    "        else:\n",
    "            matched_phrase = \"\"\n",
    "            raw_start, raw_end = (None, None)\n",
    "\n",
    "        #convert to plain Python ints (or None) so JSON can handle it\n",
    "        p_start = to_py_int_or_none(raw_start)\n",
    "        p_end = to_py_int_or_none(raw_end)\n",
    "\n",
    "        item = {\n",
    "            \"label\": label_text,\n",
    "            \"skillType\": skill_type,\n",
    "            \"score\": score_value,\n",
    "            \"matched_phrase\": matched_phrase,\n",
    "            \"char_span\": {\n",
    "                \"start\": p_start,\n",
    "                \"end\": p_end\n",
    "            } if p_start is not None and p_end is not None else None\n",
    "        }\n",
    "\n",
    "        results.append(item)\n",
    "\n",
    "    #remove duplicate labels, keep first\n",
    "    results = greedy_funk(results, key=\"label\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad1dea-c65d-400f-8bef-bf101de9bb76",
   "metadata": {},
   "source": [
    "### Unit Level matching helper\n",
    "The unit texts are matched to ESCO descriptions using cosine similarity, but return ESCO labels. Duplicates are removed, and the process runs until the top-k is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7790def3-3949-406d-b9de-56b8f4af0000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_units_to_desc_return_label_capped(units_list, E_desc, esco_lbl_df, max_matches):\n",
    "\n",
    "\n",
    "    #first we build the unit embedding matrix\n",
    "    vec_list = []\n",
    "    for text, vec, u_start, u_end in units_list:  #unpack spans as well\n",
    "        vec_list.append(vec.astype(np.float32))\n",
    "\n",
    "    U = np.vstack(vec_list)\n",
    "    U = normalize_rows(U)\n",
    "\n",
    "    #next we compute the cosine simularity\n",
    "    S = np.dot(U, E_desc.T)\n",
    "\n",
    "    n_units = S.shape[0]\n",
    "    n_esco = S.shape[1]\n",
    "\n",
    "    per_unit_window = max(50, max_matches * 3) #how many ESCO candidates we keep per unit\n",
    "    if per_unit_window > n_esco:\n",
    "        per_unit_window = n_esco\n",
    "\n",
    "    #next we build a big list of (score, unit_index, esco_index)\n",
    "    candidates = []\n",
    "\n",
    "    for i in range(n_units):\n",
    "        sim_row = S[i]  #similarities for unit i to all ESCO descriptions\n",
    "        idx_sorted = np.argsort(-sim_row) #sort descending\n",
    "        top_idx = idx_sorted[:per_unit_window]\n",
    "\n",
    "        for j in top_idx:\n",
    "            score_ij = float(sim_row[j])\n",
    "            candidates.append((score_ij, i, j))\n",
    "\n",
    "    #sort all candidates globally by score\n",
    "    candidates.sort(key=lambda x: -x[0])\n",
    "\n",
    "    #deduplicate labels and cap at max_matches\n",
    "    label_list = esco_lbl_df[\"label\"].astype(str).tolist()\n",
    "    used_labels = set()\n",
    "    out = []\n",
    "\n",
    "    for score_val, unit_idx, esco_idx in candidates:\n",
    "        lab_text = label_list[esco_idx].strip()\n",
    "        key = lab_text.lower()\n",
    "\n",
    "        if key == \"\" or key in used_labels:\n",
    "            continue\n",
    "\n",
    "        #get unit text + span for this unit\n",
    "        u_text, _, raw_start, raw_end = units_list[unit_idx]\n",
    "\n",
    "        u_start = to_py_int_or_none(raw_start)\n",
    "        u_end = to_py_int_or_none(raw_end)\n",
    "\n",
    "        item = {\n",
    "            \"label\": lab_text,\n",
    "            \"skillType\": str(esco_lbl_df.iloc[esco_idx][\"skillType\"]),\n",
    "            \"score\": float(score_val),\n",
    "            \"matched_unit\": u_text,  # original unit text\n",
    "            \"char_span\": {\n",
    "                \"start\": u_start,\n",
    "                \"end\": u_end\n",
    "            } if u_start is not None and u_end is not None else None\n",
    "        }\n",
    "\n",
    "        out.append(item)\n",
    "        used_labels.add(key)\n",
    "\n",
    "        if len(out) >= max_matches:\n",
    "            break\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81281985-ebe2-4394-8fcf-ab38a8b0fcb4",
   "metadata": {},
   "source": [
    "### Main matching algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a8b996a-7c81-4dab-86a9-482bdb06195c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished matching all selected jobs.\n",
      "Total jobs matched: 400\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "for i in range(len(jobs)):\n",
    "    jr = jobs.loc[i]\n",
    "    jid = jr[\"job_id\"]\n",
    "\n",
    "    # title and cleaned job text\n",
    "    title = jr.get(\"title\", \"\")\n",
    "    if not isinstance(title, str):\n",
    "        title = \"\"\n",
    "    title = title.strip()\n",
    "\n",
    "    tded = jr.get(\"text_deduped\", \"\")\n",
    "    if not isinstance(tded, str):\n",
    "        tded = \"\"\n",
    "    tded = tded.strip()\n",
    "\n",
    "    #phrase level\n",
    "    phrases = jobid_to_phr.get(jid, [])\n",
    "    phrase_matches_kn = []\n",
    "    phrase_matches_sk = []\n",
    "\n",
    "    if len(phrases) > 0:\n",
    "        #build matrix P from phrase embeddings\n",
    "        vec_list = []\n",
    "        phr_txt = []\n",
    "        phr_tok_list = []\n",
    "        phr_spans = []  #list of (start, end) per phrase\n",
    "\n",
    "        for p in phrases:\n",
    "            phr_txt.append(p[0]) #text\n",
    "            vec_list.append(p[1]) #embedding\n",
    "            phr_tok_list.append(p[2]) #token set\n",
    "            \n",
    "            if len(p) >= 5:\n",
    "                phr_spans.append((p[3], p[4]))\n",
    "            else:\n",
    "                phr_spans.append((None, None))\n",
    "\n",
    "        Phr_emb = np.vstack(vec_list).astype(np.float32)\n",
    "        Phr_emb = normalize_rows(Phr_emb)\n",
    "\n",
    "        #union of all phrase tokens\n",
    "        phr_tok_union = set()\n",
    "        for token_set in phr_tok_list:\n",
    "            phr_tok_union = phr_tok_union.union(token_set)\n",
    "\n",
    "        #title tokens\n",
    "        title_tok = tokenize(title, stopwords=STOP_DE)\n",
    "        title_tok = set(title_tok)\n",
    "\n",
    "        #score ESCO labels for knowledge and skill buckets\n",
    "        phrase_matches_kn = score_bucket_labels(\n",
    "            Phr_emb,\n",
    "            phr_txt,\n",
    "            phr_tok_list,\n",
    "            phr_tok_union,\n",
    "            title_tok,\n",
    "            mx_lbl_kn,\n",
    "            lbl_kn,\n",
    "            phr_spans\n",
    "        )\n",
    "\n",
    "        phrase_matches_sk = score_bucket_labels(\n",
    "            Phr_emb,\n",
    "            phr_txt,\n",
    "            phr_tok_list,\n",
    "            phr_tok_union,\n",
    "            title_tok,\n",
    "            mx_lbl_sk,\n",
    "            lbl_sk,\n",
    "            phr_spans\n",
    "        )\n",
    "\n",
    "\n",
    "    #unit level\n",
    "    units = jobid_to_units.get(jid, [])\n",
    "    unit_matches_kn = []\n",
    "    unit_matches_sk = []\n",
    "\n",
    "    if len(units) > 0:\n",
    "        unit_matches_kn = match_units_to_desc_return_label_capped(\n",
    "            units,\n",
    "            mx_desc_kn,\n",
    "            desc_kn,\n",
    "            MAX_UNIT_MATCHES\n",
    "        )\n",
    "\n",
    "        unit_matches_sk = match_units_to_desc_return_label_capped(\n",
    "            units,\n",
    "            mx_desc_sk,\n",
    "            desc_sk,\n",
    "            MAX_UNIT_MATCHES\n",
    "        )\n",
    "\n",
    "    #final_meta table (with normalized results)\n",
    "    add_percentile_norm(phrase_matches_kn)\n",
    "    add_percentile_norm(phrase_matches_sk)\n",
    "    add_percentile_norm(unit_matches_kn)\n",
    "    add_percentile_norm(unit_matches_sk)\n",
    "\n",
    "    combined = []\n",
    "\n",
    "    #phrase-knowledge matches\n",
    "    for x in phrase_matches_kn:\n",
    "        item = {\n",
    "            \"source\": \"phrase\",\n",
    "            \"bucket\": \"knowledge\",\n",
    "            \"label\": x[\"label\"],\n",
    "            \"score\": float(x[\"score\"]),\n",
    "            \"norm\": float(x.get(\"norm\", 0.0)),\n",
    "            \"context\": x.get(\"matched_phrase\", \"\"),\n",
    "            \"char_span\": x.get(\"char_span\")\n",
    "        }\n",
    "        combined.append(item)\n",
    "\n",
    "    #phrase-skill matches\n",
    "    for x in phrase_matches_sk:\n",
    "        item = {\n",
    "            \"source\": \"phrase\",\n",
    "            \"bucket\": \"skill\",\n",
    "            \"label\": x[\"label\"],\n",
    "            \"score\": float(x[\"score\"]),\n",
    "            \"norm\": float(x.get(\"norm\", 0.0)),\n",
    "            \"context\": x.get(\"matched_phrase\", \"\"),\n",
    "            \"char_span\": x.get(\"char_span\")\n",
    "        }\n",
    "        combined.append(item)\n",
    "\n",
    "    #unit-knowledge matches\n",
    "    for x in unit_matches_kn:\n",
    "        item = {\n",
    "            \"source\": \"unit\",\n",
    "            \"bucket\": \"knowledge\",\n",
    "            \"label\": x[\"label\"],\n",
    "            \"score\": float(x[\"score\"]),\n",
    "            \"norm\": float(x.get(\"norm\", 0.0)),\n",
    "            \"context\": x.get(\"matched_unit\", \"\"),\n",
    "            \"char_span\": x.get(\"char_span\")\n",
    "        }\n",
    "        combined.append(item)\n",
    "\n",
    "    #unit-skill matches\n",
    "    for x in unit_matches_sk:\n",
    "        item = {\n",
    "            \"source\": \"unit\",\n",
    "            \"bucket\": \"skill\",\n",
    "            \"label\": x[\"label\"],\n",
    "            \"score\": float(x[\"score\"]),\n",
    "            \"norm\": float(x.get(\"norm\", 0.0)),\n",
    "            \"context\": x.get(\"matched_unit\", \"\"),\n",
    "            \"char_span\": x.get(\"char_span\")\n",
    "        }\n",
    "        combined.append(item)\n",
    "\n",
    "\n",
    "    #sort by normalized score\n",
    "    combined.sort(key=lambda z: -z[\"norm\"])\n",
    "\n",
    "    best_overall = []\n",
    "    seen_labels = set()\n",
    "\n",
    "    for x in combined:\n",
    "        label_key = x[\"label\"].strip().lower()\n",
    "\n",
    "        if label_key == \"\" or label_key in seen_labels:\n",
    "            continue\n",
    "\n",
    "        #character span was already computed in preprocessing and propagated\n",
    "        span = x.get(\"char_span\")\n",
    "\n",
    "        best_item = {\n",
    "            \"source\": x[\"source\"],\n",
    "            \"bucket\": x[\"bucket\"],\n",
    "            \"label\": x[\"label\"],\n",
    "            \"score\": x[\"score\"],\n",
    "            \"norm\": x[\"norm\"],\n",
    "            \"context\": x[\"context\"],\n",
    "            \"char_span\": span\n",
    "        }\n",
    "\n",
    "        best_overall.append(best_item)\n",
    "        seen_labels.add(label_key)\n",
    "\n",
    "        if len(best_overall) >= FINAL_TOP:\n",
    "            break\n",
    "\n",
    "\n",
    "    #store summary for this job\n",
    "    rows.append({\n",
    "        \"job_id\": jid,\n",
    "        \"title\": title,\n",
    "        \"text_deduped\": tded,\n",
    "        \"phrase_matches_knowledge\": json.dumps(phrase_matches_kn, ensure_ascii=False),\n",
    "        \"phrase_matches_skills\": json.dumps(phrase_matches_sk, ensure_ascii=False),\n",
    "        \"unit_matches_knowledge\": json.dumps(unit_matches_kn, ensure_ascii=False),\n",
    "        \"unit_matches_skills\": json.dumps(unit_matches_sk, ensure_ascii=False),\n",
    "        \"final_meta\": json.dumps(best_overall, ensure_ascii=False),\n",
    "    })\n",
    "\n",
    "print(\"Finished matching all selected jobs.\")\n",
    "print(\"Total jobs matched:\", len(rows))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c752ea-f656-4537-967e-1b5b364fa63b",
   "metadata": {},
   "source": [
    "### Save matched results to SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5a499d5-d56d-4ca7-ac72-161d75d31020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result DataFrame shape: (400, 8)\n",
      "Rows written: 400\n",
      "\n",
      "Configuration that was used:\n",
      " JOB_IDS = []\n",
      " SAMPLE_JOBS = None\n",
      " TOP_K_PHRASE = 20\n",
      " MAX_UNIT_MATCHES = 20\n",
      " FINAL_TOP = 40\n",
      "\n",
      "Preview (first 3 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text_deduped</th>\n",
       "      <th>phrase_matches_knowledge</th>\n",
       "      <th>phrase_matches_skills</th>\n",
       "      <th>unit_matches_knowledge</th>\n",
       "      <th>unit_matches_skills</th>\n",
       "      <th>final_meta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Vertragshochschullehrperson/Hochschullehrperso...</td>\n",
       "      <td>Die Verwendung als Vertragshochschullehrperson...</td>\n",
       "      <td>[{\"label\": \"Beratung\", \"skillType\": \"knowledge...</td>\n",
       "      <td>[{\"label\": \"Qualitätssicherung durchführen\", \"...</td>\n",
       "      <td>[{\"label\": \"IKT-Qualitätspolitik\", \"skillType\"...</td>\n",
       "      <td>[{\"label\": \"Kontakt zur Qualitätssicherung auf...</td>\n",
       "      <td>[{\"source\": \"phrase\", \"bucket\": \"knowledge\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Vertragshochschullehrperson/Hochschullehrperso...</td>\n",
       "      <td>Die Verwendung als Vertragshochschullehrperson...</td>\n",
       "      <td>[{\"label\": \"Beratung\", \"skillType\": \"knowledge...</td>\n",
       "      <td>[{\"label\": \"Publikationspläne vorlegen\", \"skil...</td>\n",
       "      <td>[{\"label\": \"Fachkenntnisse im Bereich Ausbildu...</td>\n",
       "      <td>[{\"label\": \"mit Bildungsträgern zusammenarbeit...</td>\n",
       "      <td>[{\"source\": \"phrase\", \"bucket\": \"knowledge\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Vertragshochschullehrperson/Hochschullehrperso...</td>\n",
       "      <td>Die Verwendung als Vertragshochschullehrperson...</td>\n",
       "      <td>[{\"label\": \"Ziele für nachhaltige Entwicklung\"...</td>\n",
       "      <td>[{\"label\": \"nachhaltigen Beschaffung umsetzen\"...</td>\n",
       "      <td>[{\"label\": \"organisatorischer Aufbau\", \"skillT...</td>\n",
       "      <td>[{\"label\": \"organisatorische Richtlinien festl...</td>\n",
       "      <td>[{\"source\": \"phrase\", \"bucket\": \"knowledge\", \"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   job_id                                              title  \\\n",
       "0       1  Vertragshochschullehrperson/Hochschullehrperso...   \n",
       "1       2  Vertragshochschullehrperson/Hochschullehrperso...   \n",
       "2       3  Vertragshochschullehrperson/Hochschullehrperso...   \n",
       "\n",
       "                                        text_deduped  \\\n",
       "0  Die Verwendung als Vertragshochschullehrperson...   \n",
       "1  Die Verwendung als Vertragshochschullehrperson...   \n",
       "2  Die Verwendung als Vertragshochschullehrperson...   \n",
       "\n",
       "                            phrase_matches_knowledge  \\\n",
       "0  [{\"label\": \"Beratung\", \"skillType\": \"knowledge...   \n",
       "1  [{\"label\": \"Beratung\", \"skillType\": \"knowledge...   \n",
       "2  [{\"label\": \"Ziele für nachhaltige Entwicklung\"...   \n",
       "\n",
       "                               phrase_matches_skills  \\\n",
       "0  [{\"label\": \"Qualitätssicherung durchführen\", \"...   \n",
       "1  [{\"label\": \"Publikationspläne vorlegen\", \"skil...   \n",
       "2  [{\"label\": \"nachhaltigen Beschaffung umsetzen\"...   \n",
       "\n",
       "                              unit_matches_knowledge  \\\n",
       "0  [{\"label\": \"IKT-Qualitätspolitik\", \"skillType\"...   \n",
       "1  [{\"label\": \"Fachkenntnisse im Bereich Ausbildu...   \n",
       "2  [{\"label\": \"organisatorischer Aufbau\", \"skillT...   \n",
       "\n",
       "                                 unit_matches_skills  \\\n",
       "0  [{\"label\": \"Kontakt zur Qualitätssicherung auf...   \n",
       "1  [{\"label\": \"mit Bildungsträgern zusammenarbeit...   \n",
       "2  [{\"label\": \"organisatorische Richtlinien festl...   \n",
       "\n",
       "                                          final_meta  \n",
       "0  [{\"source\": \"phrase\", \"bucket\": \"knowledge\", \"...  \n",
       "1  [{\"source\": \"phrase\", \"bucket\": \"knowledge\", \"...  \n",
       "2  [{\"source\": \"phrase\", \"bucket\": \"knowledge\", \"...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = pd.DataFrame(rows)\n",
    "\n",
    "print(\"Result DataFrame shape:\", res.shape)\n",
    "\n",
    "conn = sqlite3.connect(str(SQLITE_PATH))\n",
    "res.to_sql(\"matched_results\", conn, if_exists=\"replace\", index=False)\n",
    "conn.close()\n",
    "\n",
    "\n",
    "print(\"Rows written:\", len(res))\n",
    "\n",
    "print(\"Configuration that was used:\")\n",
    "print(\"JOB_IDS =\", JOB_IDS)\n",
    "print(\"SAMPLE_JOBS =\", SAMPLE_JOBS)\n",
    "print(\"TOP_K_PHRASE =\", TOP_K_PHRASE)\n",
    "print(\"MAX_UNIT_MATCHES =\", MAX_UNIT_MATCHES)\n",
    "print(\"FINAL_TOP =\", FINAL_TOP)\n",
    "\n",
    "print(\"Preview (first 3 rows):\")\n",
    "display(res.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177f9ed7-f056-4fd5-aed3-4a73053a8e69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
